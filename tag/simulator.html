<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>Vikram's Blog - Simulator</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Vikram's Blog Atom Feed" />
        <link href="/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="Vikram's Blog RSS Feed" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">Vikram's Blog </a></h1>
                <nav><ul>
                    <li><a href="/category/deep-learning.html">Deep Learning</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/Self-Driving-Cars.html">Cloning driver behaviour using Convolutional Neural Networks</a></h1>
<footer class="post-info">
        <abbr class="published" title="2019-01-18T06:00:00+05:30">
                Published: Fri 18 January 2019
        </abbr>
		<br />
        <abbr class="modified" title="2019-01-18T08:30:00+05:30">
                Updated: Fri 18 January 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/vikram-iyer.html">Vikram Iyer</a>
        </address>
<p>In <a href="/category/deep-learning.html">Deep Learning</a>.</p>
<p>tags: <a href="/tag/self-driving-cars.html">Self Driving Cars</a> <a href="/tag/simulator.html">Simulator</a> <a href="/tag/behavioural-cloning.html">Behavioural Cloning</a> <a href="/tag/keras.html">Keras</a> <a href="/tag/deep-learning.html">Deep Learning</a> <a href="/tag/deep-neural-networks.html">Deep Neural Networks</a> <a href="/tag/convolutional-neural-networks.html">Convolutional Neural Networks</a> <a href="/tag/cnn.html">CNN</a> </p>
</footer><!-- /.post-info --><h1>Behavioural Cloning Project</h1>
<p><a href="http://www.udacity.com/drive"><img alt="Udacity - Self-Driving Car NanoDegree" src="https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg"></a></p>
<h2>Overview</h2>
<p>In this project, we will use deep neural networks and convolutional neural networks to clone driving behaviour. The model we train using Keras will output a steering angle to an autonomous vehicle.</p>
<p><img alt="" src="/images/p4.gif"></p>
<h2>The Project</h2>
<p>The goals / steps of this project are the following:
<em> Use the simulator to collect data of good driving behavior
</em> Design, train and validate a model that predicts a steering angle from image data
<em> Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track.
</em> Summarize the results with a written report</p>
<h2>Administrative Stuff</h2>
<p>Below are the files as required in the rubric
1. <a href="https://github.com/vikramriyer/Teach_A_Car_To_Drive_Using_Deep_Learning/blob/master/drive.py">drive.py</a>
2. <a href="https://github.com/vikramriyer/Teach_A_Car_To_Drive_Using_Deep_Learning/blob/master/model.py">model.py</a>
3. <a href="https://github.com/vikramriyer/Teach_A_Car_To_Drive_Using_Deep_Learning/blob/master/model.h5">model.h5</a> <br>
Not viewable. Please download to run the simulator.
4. <a href="https://github.com/vikramriyer/Teach_A_Car_To_Drive_Using_Deep_Learning/blob/master/run1.mp4">video</a> of the car driving itself. <br>
Not viewable. Please download to view. OR <a href="https://youtu.be/DC2Br_Sq0P4">YoutubeLink</a> to view (driver's view) directly. <br>
<strong>PS: The video quality is not the greatest but I am submitting the project anyways. Will update if I create a better one after data augmentation.</strong>
5. README <br>
This is the github readme.</p>
<h2>Steps</h2>
<h3>Data Collection</h3>
<p>The Udacity simulator was used in the following fashion
1. 3 laps recorded in the normal direction
2. 3 laps recorded in the flipped direction
3. 1 lap recorded as correction track: This recording has the car driving itself back to the road if it goes off the track</p>
<h3>Dataset summary</h3>
<p>We have 2 types of files to look at in this dataset
1. <strong>The driving_log.csv</strong> <br>
This file has information about the paths where the images (left, center, right) are stored on the disk, the steering angle, throttle, reverse, speed. <br>
For this project, we will only be focussing on steering angle and pose this problem as a regression problem. Our speed will be kept constant at 9kmh, however, at each frame, the steering angle will be predicted and the car will steer itself appropriately.
2. <strong>IMG dir</strong> <br>
This directory consists images taken using the left, center and right cameras mounted on the car. For this version of the project, I used only the center images and could train a model to successfully steer it on road for 1 lap. <br>
For the next versions, when I work on the challenge problems, I might use the left and right images to make the model robust.</p>
<p>After the thresholding we have,</p>
<div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">paths</span><span class="p">,</span> <span class="n">measurements</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># paths has the image path on the disk</span>
<span class="c1"># corresponding steering angle measurements of the images in the paths list</span>
</pre></div>


<table>
<thead>
<tr>
<th>Total</th>
<th>Train</th>
<th>Valid</th>
</tr>
</thead>
<tbody>
<tr>
<td>1642</td>
<td>1313</td>
<td>329</td>
</tr>
</tbody>
</table>
<h3>Dataset exploration and balancing using Thresholds</h3>
<p>Let's check how our data was distributed before the thresholding and what is the final statistic of the dataset.</p>
<p><strong>Before Thresholding</strong> <br>
There are a lot of entries for steering angle 0</p>
<p><img alt="" src="/images/hist.png"></p>
<p><strong>After Thresholding</strong> <br>
We can now see that the distribution looks more normal and should be good to go ahead with the preprocessing step.</p>
<p><img alt="" src="/images/hist_thresholded.png"></p>
<p><strong>Statistics of Train and Validation set</strong> <br>
After, splitting the data into train and validation set, here are the distributions for them respectively. The distribution looks identical and hence we are good to go ahead with the preprocessing and training procedure.</p>
<p><img alt="" src="/images/train_valid_dist.png"></p>
<h4>What is the thresholding step anyways?</h4>
<p>Since we have a lot of images having 0 as the steering angle, we wanted to avoid any bias and there was no use training the algo on similar set of images with the prediction variable being same. So, we set <strong>200</strong> to be a threashold and only that many images having steering angle 0 were picked <strong>randomly</strong>.</p>
<div class="highlight"><pre><span></span><span class="n">bin_threshold</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">remove_ixs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_bins</span><span class="p">):</span>
  <span class="n">temp_list</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">steering</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;steering&#39;</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">bins</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="ow">and</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;steering&#39;</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">bins</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]:</span>
      <span class="n">temp_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
  <span class="c1"># ensure that data is dropped randomly rather than a specific portion of the recording</span>
  <span class="n">temp_list</span> <span class="o">=</span> <span class="n">shuffle</span><span class="p">(</span><span class="n">temp_list</span><span class="p">)</span>
  <span class="n">temp_list</span> <span class="o">=</span> <span class="n">temp_list</span><span class="p">[</span><span class="n">bin_threshold</span><span class="p">:]</span>
  <span class="n">remove_ixs</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">temp_list</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="n">remove_ixs</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<h3>Preprocessing Data</h3>
<p>The preprocessing steps we are going to follow are listed below <br></p>
<h4>Color space selection</h4>
<p>We will be using the model proposed by NVidia that uses the left, center and right images for the bahavioural cloning project.
Convert to <strong>YUV</strong> space</p>
<div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">img_path</span><span class="p">),</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2YUV</span><span class="p">)</span>
</pre></div>


<h4>Region of Interest</h4>
<p>We can see that the scenery is actually a distraction for the model and we are better off focusing only on the road track. So, we will be slicing the part that is required and ommiting the upper half of the image.</p>
<div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">[</span><span class="mi">60</span><span class="p">:</span><span class="mi">135</span><span class="p">,:,:]</span>
</pre></div>


<h4>Smoothing the image with a gaussian filter</h4>
<p>This is usually a common step in the image preprocessing where our intent is to denoise the image of any form of clutter. As we are replicating the Nvidia model, we will use this step as mentioned in the model preprocessing stage.</p>
<div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">GaussianBlur</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>


<h4>Reshape to fit the input size of Nvidia Model (200x66x3)</h4>
<p>To avoid any form of bias as well as having to manage images differently depending only on its size is usually avoided by reshaping the input images to the same size. We can call it a standardizing step that helps us avoid unwanted computation.</p>
<div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">66</span><span class="p">))</span>
</pre></div>


<h4>Image Normalization</h4>
<p>Normalization or min-max scaling is a technique by which the values in the image are scaled to a fixed range. In our case we will fit them between 0-1. Typically in a neural network architecture, convergence is faster if the data points follow a similar distribution. The variance in the data is also reduced by using normalization after the zero-centering. This makes the computations faster as well, thus helping in faster convergence.</p>
<div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">/</span><span class="mi">255</span>
</pre></div>


<h3>Model Architecture and Training</h3>
<p>Below is the architecture of the Model, a modified version as inspired from Nvidia Model.</p>
<table>
<thead>
<tr>
<th>Layer</th>
<th>Output_Shape</th>
<th>Total_Parameters</th>
</tr>
</thead>
<tbody>
<tr>
<td>conv2D</td>
<td>31x98x24</td>
<td>5x5x24x3+24=1824</td>
</tr>
<tr>
<td>conv2D</td>
<td>14x47x36</td>
<td>5x5x36x24+36=21636</td>
</tr>
<tr>
<td>conv2D</td>
<td>5x22x48</td>
<td>5x5x48x36+48=43248</td>
</tr>
<tr>
<td>conv2D</td>
<td>3x20x64</td>
<td>3x3x64x48+64=27712</td>
</tr>
<tr>
<td>conv2D</td>
<td>1x18x64</td>
<td>3x3x64x64+64=36928</td>
</tr>
<tr>
<td>dropout</td>
<td>1x18x64</td>
<td>0</td>
</tr>
<tr>
<td>flatten</td>
<td>1152</td>
<td>0</td>
</tr>
<tr>
<td>dense</td>
<td>100</td>
<td>1152x100+100=115300</td>
</tr>
<tr>
<td>dropout</td>
<td>100</td>
<td>0</td>
</tr>
<tr>
<td>dense</td>
<td>50</td>
<td>100x50+50=5050</td>
</tr>
<tr>
<td>dense</td>
<td>10</td>
<td>total_params=50x10+10=510</td>
</tr>
<tr>
<td>dense</td>
<td>1</td>
<td>total_params=10x1+1=11</td>
</tr>
</tbody>
</table>
<p>The configurations used are:</p>
<table>
<thead>
<tr>
<th>Function</th>
<th>Type</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td>RELU</td>
<td>Activation</td>
<td>Used at the conv layers</td>
</tr>
<tr>
<td>MSE</td>
<td>Error function</td>
<td>Since this is a regression problem, we use mean squared error to predict the steering angle</td>
</tr>
<tr>
<td>ADAM</td>
<td>Optimizer</td>
<td>We use the adam optimizer with a learning rate of 0.001</td>
</tr>
</tbody>
</table>
<p>Let's find out in short what each of the layers do: <br>
<strong>Conv layer</strong> <br>
We (rather the library) use a kernel or a filter that is a matrix of values and we do simple matrix multiplication and get values that are passed on as inputs to the next layers. This operation finds out certain details about the image like edges, vertices, circles, faces, etc. These kernels are chosen at random by the library and each of these produce some form of results about the features. These kernels are</p>
<p><strong>Fully connected layer (Dense)</strong> <br>
The convolutional layers learn some low level features and to make most of the non-linearities, we use the FC layers that perform combinations of these features and find the best of these to use. This process is again done by using back propogation which learns the best of combinations.</p>
<p><strong>Dropout</strong> <br>
We randomly drop some information from the network. Though there is a experimental proof about this working well, we can in short say that this method reduces over fitting. It is a form of Regularization.</p>
<h4>Training</h4>
<p>Finally that our architecture is decided, we use the <strong>Adam optimizer</strong> for training the model. We would not go into details of how the Adam optimizer works but in short we can say that, "<strong>An optimizer in general minimizes the loss in the network.</strong>"</p>
<p>The hyperparameters:</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>EPOCHS</td>
<td>15</td>
</tr>
<tr>
<td>LEARNING RATE</td>
<td>0.001</td>
</tr>
<tr>
<td>BATCH SIZE</td>
<td>64</td>
</tr>
</tbody>
</table>
<p><img alt="" src="/images/train_val_loss_curve.png"></p>
<p>I trained for a few more epochs and did not see a possible improvement in the model and hence used 15 as the number of epochs.</p>
<h2>Discussion</h2>
<hr>
<h3>Potential Shortcomings in the Project</h3>
<ol>
<li>
<p>Failing on the challenge track <br>
When the same model is used to run the simulator on the challenge track, the model fails and falls off the track.</p>
</li>
<li>
<p>Possible overfitting <br>
As mentioned above, the model might be prone to overfitting as it fails terribly on the challenge track.</p>
</li>
</ol>
<h3>Possible Improvements</h3>
<ol>
<li>Data Augmentation <br>
The current train and validation set count is ~1300 and ~300 respectively; a very low number as far as a deep learning model is concerned. We will require a lot of more data and the simplest way can be augmenting the data by using flipping, rotations. etc. This way, the volume of data is increased with some variations added to what the network has to learn to make sure it stays on track.</li>
<li>Recording more data <br>
It will be useful to record a few laps on the challenge track as well as that might help avoid the overfitting. The model will then generalize better to the challenge track as well and probably to other types of terrians.</li>
<li>Using the other features available (Throttle, Speed, etc) <br>
We have a single image pipeline where we use the image characteristics as our features. It is a possibility that we use the other features available in the driving_log.csv which can aide our model.</li>
</ol>                </article>
            </aside><!-- /#featured -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                            <li><a href="#">You can modify those links in your config file</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>
                            <li><a href="/feeds/all.rss.xml" type="application/rss+xml" rel="alternate">rss feed</a></li>

                            <li><a href="#">You can add links in your config file</a></li>
                            <li><a href="#">Another social link</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>